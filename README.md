# IoT Sensor Data Collection and Analysis System

This project implements a comprehensive system for collecting, storing, and analyzing data from virtual IoT sensors. It is designed to demonstrate proficiency in various modern data engineering and web development technologies, including Flask for API development, SQLite for persistent data storage, Hadoop for batch processing and statistical analysis, and Docker for full system containerization and deployment.

## Objective

The primary objective of this system is to:
* Collect data from multiple independent virtual IoT sensors.
* Store sensor measurements efficiently in a SQLite database.
* Provide a RESTful API using Flask for data ingestion and retrieval.
* Perform batch processing on collected data using Hadoop Streaming to compute statistical summaries (average, minimum, and maximum values).
* Utilize Docker to containerize all components, ensuring a scalable and isolated deployment environment for the web application, data aggregation service, and individual virtual sensors.

## Core Components & Technologies

The system is built around several key components, each serving a distinct role:

### 1. Data Collection & Storage (Python, SQLite3)

* **Virtual Sensors:** Multiple independent virtual sensors, each running in a separate Docker container, periodically generate and send data to the Flask API.
* **Database Schema:** Data is stored in an SQLite3 database (`iot_data.db`) with three linked tables (`Sensor`, `SensorType`, `Measurement`) ensuring data integrity via primary and foreign keys.
    * `sensor_id`: Unique integer identifier for each virtual sensor.
    * `sensor_location`: Latitude and longitude, containing the position of the virtual sensor.
    * `sensor_type` & `unit`: Defines the type of measurement (e.g., Temperature, Pressure, Air Quality, CO2) and its corresponding unit. Distinct sensors can be of the same type, and a single sensor can collect various types of data.
        * Temperature Sensor (Â°C)
        * Pressure Sensor (hPa)
        * Air Quality Sensor (PM10)
        * CO2 Sensor (ppm)
    * `timestamp` (UTC format): Generated by the virtual sensor, indicating when the measurement was recorded.
    * `value` (numeric real): Contains the numeric real value of the measurement.

### 2. Web Application (Flask)

A Flask-based API serves as the central hub for data interaction, providing the following endpoints:

* **`/store` (POST):**
    * **Purpose:** Ingests new sensor data records into the SQLite database.
    * **Method:** POST
    * **Payload:** Expects JSON data matching the sensor data fields (`sensor_id`, `timestamp`, `type`, `value`, `location_lat`, `location_lon`).
* **`/retrieve?sensor_id=X&start_time=A&end_time=B` (GET):**
    * **Purpose:** Fetches measurements recorded by a specific sensor (`X`) within a given time range (`A` to `B`).
    * **Method:** GET
    * **Parameters:** `sensor_id`, `start_time` (UTC `YYYY-MM-DD HH:MM:SS`), `end_time` (UTC `YYYY-MM-DD HH:MM:SS`).
    * **Response:** Data is presented in a structured tabular format on the browser, grouped by sensor type and chronologically ordered.
* **`/fetch?type=T` (GET):**
    * **Purpose:** Allows users to download a plain text file containing all measurements stored in the database for the requested sensor type (`T`). This output serves as input for the Hadoop aggregation.
    * **Method:** GET
    * **Parameters:** `type` (e.g., "Temperature", "Pressure").
    * **Response:** Plain text file where each line is `timestamp,value,latitude,longitude`.

### 3. Data Aggregation (Hadoop Streaming, Python)

Hadoop Streaming is used for batch processing of sensor data to compute statistical summaries (minimum, maximum, and average values).

* **Input:** Plain-text data fetched from the `/fetch` endpoint (e.g., `sensor_data.txt`).
* **Filtering:** The computation only considers values whose location falls within a given distance `D` (in kilometers) from a specified geographic point (`X`, `Y`).
* **Distance Calculation:** The Haversine formula is used to calculate distances between sensor locations and the specified point.
* **Flexibility:** Parameters `D`, `X`, and `Y` are passed to the Hadoop job scripts (`hadoop_mapper.py`) as command-line arguments, avoiding hardcoding.
* **Scripts:**
    * `hadoop_mapper.py`: Filters data based on geographic location and outputs only the relevant sensor values.
    * `hadoop_reducer.py`: Receives the filtered values and computes the minimum, maximum, and average.

### 4. Deployment & Scalability (Docker, Docker Compose)

The entire system is containerized and orchestrated using Docker Compose, ensuring easy deployment, isolation, and efficient communication between components.

* **Virtual Sensors:** Each sensor (`sensor.py`) runs in its own independent container. A common image is used (built from `Dockerfile`), with environment variables (`SENSOR_ID`, `SENSOR_TYPE`, `API_URL`) specifying unique parameters for each instance.
* **Web Application:** The Flask application runs in a dedicated container (built from `Dockerfile-flask`).
* **Hadoop Cluster:** A Hadoop environment is set up in a separate container using a pre-built image (`bde2023/hadoop-resourcemanager-aarch64:2.0.0-hadoop3.3.5-java11`).
* **Network Isolation:** All components are deployed on a dedicated Docker virtual network (`iot_network`) for secure and efficient inter-container communication.

## Project Structure (Key Files)

* `app.py`: Main Flask web application logic and API endpoints.
* `database_setup.py`: Script for setting up the SQLite database schema and populating initial data.
* `sensor.py`: Python script simulating a virtual IoT sensor, sending data to the Flask API.
* `hadoop_mapper.py`: Hadoop Streaming mapper script for processing data and filtering by location.
* `hadoop_reducer.py`: Hadoop Streaming reducer script for calculating min, max, and average.
* `docker-compose.yml`: Defines and orchestrates all the services (Flask app, virtual sensors, Hadoop) using Docker containers and networks.
* `Dockerfile-flask`: Dockerfile for building the Flask application image.
* `Dockerfile`: Dockerfile for building the virtual sensor image.
* `iot_data.db`: SQLite database file.
* `sensor_data.txt`: Example raw sensor data used for testing/input.

## How to Run

To get this system up and running on your local machine:

### Prerequisites

* **Docker Desktop:** Ensure Docker is installed and running on your system. This includes Docker Engine and Docker Compose. You can download it from [docker.com](https://www.docker.com/products/docker-desktop/).
* **Git:** For cloning the repository.

### Steps

1.  **Clone the Repository:**
    Open your terminal or command prompt and clone this repository to your local machine:

    ```bash
    git clone [https://github.com/your-username/your-repository-name.git](https://github.com/your-username/your-repository-name.git)
    cd your-repository-name # Navigate into the project directory
    ```
    (Replace `your-username/your-repository-name.git` with the actual URL of your repository.)

2.  **Place Hadoop Scripts:**
    Ensure `hadoop_mapper.py` and `hadoop_reducer.py` are in your project's root directory, or in a location accessible by the Hadoop container.

3.  **Build and Run Docker Containers:**
    From the root directory of the cloned project (where `docker-compose.yml` is located), run the following command to build the Docker images and start all the services:

    ```bash
    docker-compose up --build -d
    ```
    * `--build`: Builds the Docker images before starting containers (important for the first run or after code changes).
    * `-d`: Runs the containers in detached mode (in the background).

4.  **Verify Services (Optional):**
    You can check if all containers are running using:
    ```bash
    docker-compose ps
    ```
    You should see `flask_app`, `sensor1`, `sensor2`, and `hadoop` (and possibly more sensors if defined in `docker-compose.yml`) in a healthy state.

5.  **Interact with the Flask API:**
    * **Store Data:** Virtual sensors will automatically start sending data to the Flask app. You can observe Flask app logs:
        ```bash
        docker-compose logs flask_app
        ```
    * **Retrieve Recent Data:** Open your web browser and visit:
        `http://localhost:5000/retrieve?sensor_id=1&start_time=2025-01-01%2000:00:00&end_time=2025-12-31%2023:59:59`
        (Adjust `sensor_id`, `start_time`, and `end_time` as needed to match your generated data.)
    * **Fetch Data as Plain Text:** To get data for Hadoop processing, open your web browser and visit:
        `http://localhost:5000/fetch?type=Temperature`
        (Replace `Temperature` with other sensor types like `Pressure`, `Air Quality`, `CO2`. You will need to save this output to a file, e.g., `input_data.txt`, for the Hadoop job.)

6.  **Run Hadoop Aggregation Job:**
    To run a Hadoop job for data aggregation, you'll need to use `docker exec` to run commands inside the `hadoop` container.

    **Example Steps for Hadoop Job:**

    a. **Prepare Input Data:** Save the output from the `/fetch` endpoint to a file (e.g., `input_data.txt`). Then, copy this file into the Hadoop container's HDFS.
    ```bash
    # Assuming 'input_data.txt' is in your project root
    docker cp input_data.txt hadoop:/tmp/input_data.txt
    docker exec -it hadoop hdfs dfs -mkdir -p /user/hadoop/input
    docker exec -it hadoop hdfs dfs -put /tmp/input_data.txt /user/hadoop/input/
    ```

    b. **Execute Hadoop Streaming Job:**
    ```bash
    # Replace <lat_center>, <lon_center>, <distance_km> with your desired values.
    # For example, to filter for sensors within 50 km of Tirana (41.3275, 19.8189):
    # lat_center=41.3275, lon_center=19.8189, distance_km=50

    docker exec -it hadoop hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
        -file /app/hadoop_mapper.py \
        -mapper "/usr/bin/python3 hadoop_mapper.py <lat_center> <lon_center> <distance_km>" \
        -file /app/hadoop_reducer.py \
        -reducer "/usr/bin/python3 hadoop_reducer.py" \
        -input /user/hadoop/input/input_data.txt \
        -output /user/hadoop/output/
    ```
    * **Note:** You might need to adjust the paths to `hadoop_mapper.py` and `hadoop_reducer.py` (`/app/`) if they are mounted differently in your Hadoop container. Also, ensure the `hadoop_mapper.py` executable bit is set (e.g., `chmod +x hadoop_mapper.py` if running directly, though `python3` command handles it).

    c. **View Results:**
    ```bash
    docker exec -it hadoop hdfs dfs -cat /user/hadoop/output/part-00000
    ```

7.  **Stop and Clean Up:**
    When you're done, you can stop and remove all Docker containers, networks, and volumes defined in `docker-compose.yml`:
    ```bash
    docker-compose down
    ```

## Notes

* The system uses the Haversine formula for distance calculations
* Sensor locations are specified in latitude and longitude
* Data is persisted in a SQLite database
````
